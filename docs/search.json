[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Diabetes is a chronic disease affecting more than 10% of adults worldwide. It is characterized by the body’s inability to produce or properly use insulin, a hormone that regulates blood sugar. Diabetes is a lifelong condition that often leaves a person reliant on external insulin sources or other glucose-lowering treatments, and it can predispose individuals to worse health outcomes. As with most chronic diseases, early diagnosis and prevention are vital to reducing health risks.\nThe dataset used here is a cleaned version of the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) survey. As the name suggests, it collects information on health-related risk factors, such as age, prior health conditions, BMI, and behaviors like smoking and drinking, along with whether a respondent has diabetes or prediabetes. We can use this information to explore correlations between these factors and diabetes status and to build predictive models for identifying people at increased risk. This document will create a model to predict if a person currently has diabetes or prediabetes based on their current behavior.\nNote: There are multiple types of diabetes, but the BRFSS data does not distinguish among them. The only form of diabetes that is preventable through behavioral or lifestyle changes is Type II diabetes. Prediabetes is also specifically associated with Type II diabetes. Therefore, this dataset is most relevant to studying Type II diabetes, which accounts for roughly 90% of all diabetes cases.\nThe variables recorded in this dataset are:\n\nDiabetes_binary: Diabetes or prediabetes\nHighBP: High Blood Pressure\nHighChol: High Cholesterol\nCholCheck: Had a cholesterol check in the last 5 years\nBMI: Body mass index weight(kg) / height(m)^2\nSmoker: Has smoked at least 100 cigarettes total\nStroke: Has had a prior stroke\nHeartDiseaseorAttack: Has been diagnosed with heart disease or has had an attack\nPhysActivity: Has done physical activity in the past 30 days (not including work)\nFruits: Consumes fruit at least once a day\nVeggies: Consumes vegetables at least once a day\nHvyAlcoholConsump: Heavy alcohol consumption (for adults men &gt;= 14, women &gt;= 7 drinks per week)\nAnyHealthcare: Any kind of heath care coverage\nNoDocbcCost: Needed to, but could not afford to see a doctor in the past 12 months\nGenHlth: 1-5 opinion scale on general health (1 = excellent)\nMentHlth: number of poor mental health days in past 30 days\nPhysHlth: number of physical illness or injury days in past 30 days\nDiffWalk: Has difficulty walking or climbing stairs\nSex: Male or Female\nAge: 13 level age category of (approximately) 5 year gaps. Ages 18-80+\nEducation: Highest education a person has received\nIncome: Annual Income categories. Less than 10,000 to 75,000+"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "Diabetes is a chronic disease affecting more than 10% of adults worldwide. It is characterized by the body’s inability to produce or properly use insulin, a hormone that regulates blood sugar. Diabetes is a lifelong condition that often leaves a person reliant on external insulin sources or other glucose-lowering treatments, and it can predispose individuals to worse health outcomes. As with most chronic diseases, early diagnosis and prevention are vital to reducing health risks.\nThe dataset used here is a cleaned version of the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) survey. As the name suggests, it collects information on health-related risk factors, such as age, prior health conditions, BMI, and behaviors like smoking and drinking, along with whether a respondent has diabetes or prediabetes. We can use this information to explore correlations between these factors and diabetes status and to build predictive models for identifying people at increased risk. This document will create a model to predict if a person currently has diabetes or prediabetes based on their current behavior.\nNote: There are multiple types of diabetes, but the BRFSS data does not distinguish among them. The only form of diabetes that is preventable through behavioral or lifestyle changes is Type II diabetes. Prediabetes is also specifically associated with Type II diabetes. Therefore, this dataset is most relevant to studying Type II diabetes, which accounts for roughly 90% of all diabetes cases.\nThe variables recorded in this dataset are:\n\nDiabetes_binary: Diabetes or prediabetes\nHighBP: High Blood Pressure\nHighChol: High Cholesterol\nCholCheck: Had a cholesterol check in the last 5 years\nBMI: Body mass index weight(kg) / height(m)^2\nSmoker: Has smoked at least 100 cigarettes total\nStroke: Has had a prior stroke\nHeartDiseaseorAttack: Has been diagnosed with heart disease or has had an attack\nPhysActivity: Has done physical activity in the past 30 days (not including work)\nFruits: Consumes fruit at least once a day\nVeggies: Consumes vegetables at least once a day\nHvyAlcoholConsump: Heavy alcohol consumption (for adults men &gt;= 14, women &gt;= 7 drinks per week)\nAnyHealthcare: Any kind of heath care coverage\nNoDocbcCost: Needed to, but could not afford to see a doctor in the past 12 months\nGenHlth: 1-5 opinion scale on general health (1 = excellent)\nMentHlth: number of poor mental health days in past 30 days\nPhysHlth: number of physical illness or injury days in past 30 days\nDiffWalk: Has difficulty walking or climbing stairs\nSex: Male or Female\nAge: 13 level age category of (approximately) 5 year gaps. Ages 18-80+\nEducation: Highest education a person has received\nIncome: Annual Income categories. Less than 10,000 to 75,000+"
  },
  {
    "objectID": "EDA.html#loading-data",
    "href": "EDA.html#loading-data",
    "title": "EDA",
    "section": "Loading Data",
    "text": "Loading Data\nLibraries\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nImporting Data from Local Drive\n\ndf &lt;- read_csv(\"Data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking missingness from dataset. There should not be any missing values according to the creator of this cleaned data.\n\ncolSums(is.na(df))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nChanging all categorical variables to factors.\n\ndf &lt;- df |&gt;\n  mutate(across(-c(BMI, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income),\n                ~factor(.x, levels = c(0, 1), labels = c(\"No\", \"Yes\")))) |&gt;\n  mutate(GenHlth = ordered(GenHlth, levels = c(1, 2, 3, 4, 5), label=c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"))) |&gt;\n  mutate(Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\"))) |&gt;\n  mutate(Age = ordered(Age,\n         levels = 1:13, \n         labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))) |&gt;\n  mutate(Education = ordered(Education, levels = 1:6,\n         labels = c(\"Never attended school or only kindergarten\", \"Elementary\", \"Some high school\", \"High school graduate\", \"Some college\", \"College graduate\"))) |&gt;\n  mutate(Income = ordered(Income, levels = 1:8, \n         labels = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\", \"$20,000-$25,000\", \"$25,000-$35,000\", \"$35,000-$50,000\", \"$50,000-$75,000\", \"&gt;$75,000\")))"
  },
  {
    "objectID": "EDA.html#data-exploration",
    "href": "EDA.html#data-exploration",
    "title": "EDA",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nSingle Variable\nOne-way tables for categorical data. We are viewing the distribution of each of our variables.\n\nlapply(names(df |&gt; select(-c(BMI, PhysHlth, MentHlth))), function(x) tabyl(df, x))\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(x)\n\n  # Now:\n  data %&gt;% select(all_of(x))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n[[1]]\n Diabetes_binary      n  percent\n              No 218334 0.860667\n             Yes  35346 0.139333\n\n[[2]]\n HighBP      n   percent\n     No 144851 0.5709989\n    Yes 108829 0.4290011\n\n[[3]]\n HighChol      n   percent\n       No 146089 0.5758791\n      Yes 107591 0.4241209\n\n[[4]]\n CholCheck      n   percent\n        No   9470 0.0373305\n       Yes 244210 0.9626695\n\n[[5]]\n Smoker      n   percent\n     No 141257 0.5568314\n    Yes 112423 0.4431686\n\n[[6]]\n Stroke      n   percent\n     No 243388 0.9594292\n    Yes  10292 0.0405708\n\n[[7]]\n HeartDiseaseorAttack      n    percent\n                   No 229787 0.90581441\n                  Yes  23893 0.09418559\n\n[[8]]\n PhysActivity      n   percent\n           No  61760 0.2434563\n          Yes 191920 0.7565437\n\n[[9]]\n Fruits      n   percent\n     No  92782 0.3657442\n    Yes 160898 0.6342558\n\n[[10]]\n Veggies      n   percent\n      No  47839 0.1885801\n     Yes 205841 0.8114199\n\n[[11]]\n HvyAlcoholConsump      n    percent\n                No 239424 0.94380322\n               Yes  14256 0.05619678\n\n[[12]]\n AnyHealthcare      n    percent\n            No  12417 0.04894749\n           Yes 241263 0.95105251\n\n[[13]]\n NoDocbcCost      n    percent\n          No 232326 0.91582308\n         Yes  21354 0.08417692\n\n[[14]]\n   GenHlth     n    percent\n excellent 45299 0.17856749\n very good 89084 0.35116682\n      good 75646 0.29819458\n      fair 31570 0.12444812\n      poor 12081 0.04762299\n\n[[15]]\n DiffWalk      n   percent\n       No 211005 0.8317763\n      Yes  42675 0.1682237\n\n[[16]]\n    Sex      n   percent\n female 141974 0.5596578\n   male 111706 0.4403422\n\n[[17]]\n         Age     n    percent\n       18-24  5700 0.02246925\n       25-29  7598 0.02995112\n       30-34 11123 0.04384658\n       35-39 13823 0.05448991\n       40-44 16157 0.06369048\n       45-49 19819 0.07812599\n       50-54 26314 0.10372911\n       55-59 30832 0.12153895\n       60-64 33244 0.13104699\n       65-69 32194 0.12690792\n       70-74 23533 0.09276648\n       75-79 15980 0.06299275\n 80 or older 17363 0.06844450\n\n[[18]]\n                                  Education      n      percent\n Never attended school or only kindergarten    174 0.0006859035\n                                 Elementary   4043 0.0159374015\n                           Some high school   9478 0.0373620309\n                       High school graduate  62750 0.2473588773\n                               Some college  69910 0.2755834122\n                           College graduate 107325 0.4230723746\n\n[[19]]\n          Income     n    percent\n        &lt;$10,000  9811 0.03867471\n $10,000-$15,000 11783 0.04644828\n $15,000-$20,000 15994 0.06304793\n $20,000-$25,000 20135 0.07937165\n $25,000-$35,000 25883 0.10203012\n $35,000-$50,000 36470 0.14376380\n $50,000-$75,000 43219 0.17036818\n        &gt;$75,000 90385 0.35629533\n\n\nLooking at these distributions, the frequencies within education and income are too unbalanced. It is possible that the training/test split might miss a category. I am going to combine the lower categories to account for this. There are quite a few variables other unbalanced variables, where less than 5% of observations have a value. However, we have so much data, that the splits for these should not be a problem and tree models are good at handling skewness.\nAdditionally, there are a few variables (like alcohol consumption, or physical health) that are influenced by a person having diabetes. (Alcohol has a lot of sugar, diabetes might make you ill) In the scenario where we are using behaviors to predict the chance of developing diabetes, these variables might not be good to include in the model. However, we are trying to predict if the person already has diabetes based on their current habits. Therefore, all of the predictor variables will stay and the model decide which are important.\nConsolidating Education and Income\n\ndf &lt;- df |&gt;\n  mutate(\n    Education = fct_collapse(Education,\n      \"Up to some high school\" = c(\"Never attended school or only kindergarten\", \"Elementary\", \"Some high school\")),\n    Income = fct_collapse(Income, \"&lt;$20,000\" = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\")))\n\nChecking that the consolidation worked\n\ntabyl(df, Education)\n\n              Education      n    percent\n Up to some high school  13695 0.05398534\n   High school graduate  62750 0.24735888\n           Some college  69910 0.27558341\n       College graduate 107325 0.42307237\n\ntabyl(df, Income)\n\n          Income     n    percent\n        &lt;$20,000 37588 0.14817092\n $20,000-$25,000 20135 0.07937165\n $25,000-$35,000 25883 0.10203012\n $35,000-$50,000 36470 0.14376380\n $50,000-$75,000 43219 0.17036818\n        &gt;$75,000 90385 0.35629533\n\n\nViewing the distribution of numeric data.\n\npar(mfrow=c(1, 3))\nhist(df$BMI)\nhist(df$PhysHlth)\nhist(df$MentHlth)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nAll of these variables are skewed right. If we were not using tree models, I would consider taking the log to center the values. The physical and mental health variables contain a lot of zeros. We need to consider the distribution of the non-zero days by themselves\n\npar(mfrow=c(1, 2))\nhist(df$PhysHlth[df$PhysHlth &gt; 0])\nhist(df$MentHlth[df$MentHlth &gt; 0])\n\n\n\n\n\n\n\npar(mfrow=c(1, 1))\n\nNumeric Data summaries\n\nsummary(df |&gt; select(BMI, MentHlth, PhysHlth))\n\n      BMI           MentHlth         PhysHlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n\n\n\nMulti Variable\nNow we are going to see how all of the variables are in relation to the response variable (whether or not a person has diabetes/prediabetes).\n\nplot_list &lt;- vector(mode = \"list\", length = length(df))\n\nfor(i in 1:length(df)){\n  if(!is.numeric(df[[i]])){\n    p &lt;- ggplot(data=df, aes(x=!!sym(names(df)[i]), fill = Diabetes_binary)) +\n      geom_bar() +\n      labs(title = names(df)[i], x = NULL, y = NULL) +\n      theme(legend.position = \"none\")\n  }else{\n    p &lt;- ggplot(data=df, aes(x=!!sym(names(df)[i]), fill = Diabetes_binary)) +\n      geom_density(alpha=.5) +\n      labs(title = names(df)[i], x = NULL, y = NULL) +\n      theme(legend.position = \"none\")\n  }\n  plot_list[[i]] &lt;- p\n}\n\ngridExtra::grid.arrange(grobs = plot_list, nrows=6)\n\n\n\n\n\n\n\n\nThese plots show us some variables of interest: Age, BMI, HighHP, HighChol, and GenHlth. These variables have a different distribution depending on the response variable, and will likely be important in any models. There are several variables where it appears as though there are not observations in the intersection of a value and diabetes. If this is the case, there is an argument for removing this as a predictor since it offers little information. Let us investigate this numerically.\nNumerically viewing intersection with Tables\n\ncross_tabs &lt;- lapply(names(df |&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary))), function(x) xtabs(~ df[[x]] + df$Diabetes_binary))\n\nnames(cross_tabs) &lt;- names(df|&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary)))\ncross_tabs\n\n$HighBP\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  136109   8742\n    Yes  82225  26604\n\n$HighChol\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  134429  11660\n    Yes  83905  23686\n\n$CholCheck\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No    9229    241\n    Yes 209105  35105\n\n$Smoker\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  124228  17029\n    Yes  94106  18317\n\n$Stroke\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  211310  32078\n    Yes   7024   3268\n\n$HeartDiseaseorAttack\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  202319  27468\n    Yes  16015   7878\n\n$PhysActivity\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No   48701  13059\n    Yes 169633  22287\n\n$Fruits\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No   78129  14653\n    Yes 140205  20693\n\n$Veggies\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No   39229   8610\n    Yes 179105  26736\n\n$HvyAlcoholConsump\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  204910  34514\n    Yes  13424    832\n\n$AnyHealthcare\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No   10995   1422\n    Yes 207339  33924\n\n$NoDocbcCost\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  200722  31604\n    Yes  17612   3742\n\n$GenHlth\n           df$Diabetes_binary\ndf[[x]]        No   Yes\n  excellent 44159  1140\n  very good 82703  6381\n  good      62189 13457\n  fair      21780  9790\n  poor       7503  4578\n\n$DiffWalk\n       df$Diabetes_binary\ndf[[x]]     No    Yes\n    No  188780  22225\n    Yes  29554  13121\n\n$Sex\n        df$Diabetes_binary\ndf[[x]]      No    Yes\n  female 123563  18411\n  male    94771  16935\n\n$Age\n             df$Diabetes_binary\ndf[[x]]          No   Yes\n  18-24        5622    78\n  25-29        7458   140\n  30-34       10809   314\n  35-39       13197   626\n  40-44       15106  1051\n  45-49       18077  1742\n  50-54       23226  3088\n  55-59       26569  4263\n  60-64       27511  5733\n  65-69       25636  6558\n  70-74       18392  5141\n  75-79       12577  3403\n  80 or older 14154  3209\n\n$Education\n                        df$Diabetes_binary\ndf[[x]]                     No   Yes\n  Up to some high school 10169  3526\n  High school graduate   51684 11066\n  Some college           59556 10354\n  College graduate       96925 10400\n\n$Income\n                 df$Diabetes_binary\ndf[[x]]              No   Yes\n  &lt;$20,000        28551  9037\n  $20,000-$25,000 16081  4054\n  $25,000-$35,000 21379  4504\n  $35,000-$50,000 31179  5291\n  $50,000-$75,000 37954  5265\n  &gt;$75,000        83190  7195\n\n\nThere are observations in each intersection, so we will keep the predictors\nViewing the intersections with numerical summaries\n\ndf |&gt; select(c(Diabetes_binary, BMI, PhysHlth, MentHlth)) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = c(BMI, PhysHlth, MentHlth), \n      .fns = list(\n        min  = min,\n        q1   = ~quantile(., 0.25),      \n        med  = median,                  \n        mean = mean,\n        q3   = ~quantile(., 0.75),     \n        max  = max))) |&gt;\n  pivot_longer(\n    cols = -Diabetes_binary,\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) |&gt;\n\n  separate_wider_delim(\n    cols = Metric,\n    delim = \"_\",\n    names = c(\"Feature\", \"Statistic\")\n  ) |&gt;\n  \n  pivot_wider(\n    id_cols = c(Diabetes_binary, Statistic), \n    names_from = Feature, \n    values_from = Value,\n    names_glue = \"{.value}_{.name}\" \n  )\n\n# A tibble: 12 × 5\n   Diabetes_binary Statistic Value_BMI Value_PhysHlth Value_MentHlth\n   &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 No              min            12             0              0   \n 2 No              q1             24             0              0   \n 3 No              med            27             0              0   \n 4 No              mean           27.8           3.64           2.98\n 5 No              q3             31             2              2   \n 6 No              max            98            30             30   \n 7 Yes             min            13             0              0   \n 8 Yes             q1             27             0              0   \n 9 Yes             med            31             1              0   \n10 Yes             mean           31.9           7.95           4.46\n11 Yes             q3             35            15              3   \n12 Yes             max            98            30             30   \n\n\nViewing the distribution of the mental and physical health variables without the zero values.\n\ndf |&gt; select(c(Diabetes_binary, PhysHlth)) |&gt;\n  filter(PhysHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = PhysHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary PhysHlth_med PhysHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          10.6\n2 Yes                       14          15.1\n\ndf |&gt; select(c(Diabetes_binary, MentHlth)) |&gt;\n  filter(MentHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = MentHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary MentHlth_med MentHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          9.84\n2 Yes                       10         13.2 \n\n\nThese means and medians seem significant. In the next page, we can see if they are still strong predictors even with the zero values added back in.\nNow that we have explored what the data looks like, we can begin to fit models. The next page will use this data to build a model that predicts if a person has diabetes/prediabetes based on their behavior.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Diabetes is a chronic disease affecting more than 10% of adults worldwide. It is characterized by the body’s inability to produce or properly use insulin, a hormone that regulates blood sugar. Diabetes is a lifelong condition that often leaves a person reliant on external insulin sources or other glucose-lowering treatments, and it can predispose individuals to worse health outcomes. As with most chronic diseases, early diagnosis and prevention are vital to reducing health risks.\nThe dataset used here is a cleaned version of the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) survey. As the name suggests, it collects information on health-related risk factors, such as age, prior health conditions, BMI, and behaviors like smoking and drinking, along with whether a respondent has diabetes or prediabetes. This document will create a model to predict if a person currently has diabetes or prediabetes based on their current behavior."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "Diabetes is a chronic disease affecting more than 10% of adults worldwide. It is characterized by the body’s inability to produce or properly use insulin, a hormone that regulates blood sugar. Diabetes is a lifelong condition that often leaves a person reliant on external insulin sources or other glucose-lowering treatments, and it can predispose individuals to worse health outcomes. As with most chronic diseases, early diagnosis and prevention are vital to reducing health risks.\nThe dataset used here is a cleaned version of the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) survey. As the name suggests, it collects information on health-related risk factors, such as age, prior health conditions, BMI, and behaviors like smoking and drinking, along with whether a respondent has diabetes or prediabetes. This document will create a model to predict if a person currently has diabetes or prediabetes based on their current behavior."
  },
  {
    "objectID": "Modeling.html#loading-data",
    "href": "Modeling.html#loading-data",
    "title": "Modeling",
    "section": "Loading Data",
    "text": "Loading Data\nLibraries\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.1      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ readr     2.1.6     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(yardstick)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nLoading data and doing same manipulations as in the EDA file. We are changing categorical variables to factors and giving the factors meaningful names. We are also consolidating the Education and Income variables to make them more balanced\n\ndf &lt;- read_csv(\"Data/diabetes_binary_health_indicators_BRFSS2015.csv\")|&gt;\n  mutate(across(-c(BMI, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income),\n                ~factor(.x, levels = c(0, 1), labels = c(\"No\", \"Yes\")))) |&gt;\n  mutate(GenHlth = ordered(GenHlth, levels = c(1, 2, 3, 4, 5), label=c(\"excellent\", \"very good\", \"good\", \"fair\", \"poor\"))) |&gt;\n  mutate(Sex = factor(Sex, levels = c(0, 1), labels = c(\"female\", \"male\"))) |&gt;\n  mutate(Age = ordered(Age,\n         levels = 1:13, \n         labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\", \"80 or older\"))) |&gt;\n  mutate(Education = ordered(Education, levels = 1:6,\n         labels = c(\"Never attended school or only kindergarten\", \"Elementary\", \"Some high school\", \"High school graduate\", \"Some college\", \"College graduate\"))) |&gt;\n  mutate(Income = ordered(Income, levels = 1:8, \n         labels = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\", \"$20,000-$25,000\", \"$25,000-$35,000\", \"$35,000-$50,000\", \"$50,000-$75,000\", \"&gt;$75,000\")))|&gt;\n  mutate(\n    Education = fct_collapse(Education,\n      \"Up to some high school\" = c(\"Never attended school or only kindergarten\", \"Elementary\", \"Some high school\")),\n    Income = fct_collapse(Income, \"&lt;$20,000\" = c(\"&lt;$10,000\", \"$10,000-$15,000\", \"$15,000-$20,000\")))\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Modeling.html#creating-splits",
    "href": "Modeling.html#creating-splits",
    "title": "Modeling",
    "section": "Creating Splits",
    "text": "Creating Splits\nUsing tidymodels framework to split data into test and training sets. We are stratifying based on the response variable, Diabetes_binary. Then, we are further splitting the data so that we can cross-validate it later. The seed is set for reproducibility\n\nset.seed(222)\n\ndata_split &lt;- initial_split(df, prop = .7, strata = \"Diabetes_binary\")\n\n#Create data frames for the two sets\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n#Create cross validation folds\ncv_folds &lt;- vfold_cv(train_data, 5)\n\nMaking sure the test and train data have somewhat equal distributions. We are paying special attention to the highly unbalanced variables like CholCheck and Smoking.\n\ncross_tabs &lt;- lapply(names(train_data |&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary))), function(x) xtabs(~ train_data[[x]] + train_data $Diabetes_binary))\n\nnames(cross_tabs) &lt;- names(train_data |&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary)))\ncross_tabs\n\n$HighBP\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n            No  95115  6147\n            Yes 57718 18595\n\n$HighChol\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n            No  94027  8175\n            Yes 58806 16567\n\n$CholCheck\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No    6421    171\n            Yes 146412  24571\n\n$Smoker\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n            No  86790 11891\n            Yes 66043 12851\n\n$Stroke\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No  147946  22429\n            Yes   4887   2313\n\n$HeartDiseaseorAttack\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No  141552  19192\n            Yes  11281   5550\n\n$PhysActivity\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No   34203   9170\n            Yes 118630  15572\n\n$Fruits\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n            No  54726 10295\n            Yes 98107 14447\n\n$Veggies\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No   27496   6004\n            Yes 125337  18738\n\n$HvyAlcoholConsump\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No  143476  24155\n            Yes   9357    587\n\n$AnyHealthcare\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No    7713    972\n            Yes 145120  23770\n\n$NoDocbcCost\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No  140459  22144\n            Yes  12374   2598\n\n$GenHlth\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n      excellent 30821   812\n      very good 57795  4506\n      good      43613  9374\n      fair      15378  6861\n      poor       5226  3189\n\n$DiffWalk\n               train_data$Diabetes_binary\ntrain_data[[x]]     No    Yes\n            No  131971  15568\n            Yes  20862   9174\n\n$Sex\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n         female 86549 12873\n         male   66284 11869\n\n$Age\n               train_data$Diabetes_binary\ntrain_data[[x]]    No   Yes\n    18-24        3928    43\n    25-29        5153    99\n    30-34        7522   231\n    35-39        9225   432\n    40-44       10634   722\n    45-49       12595  1193\n    50-54       16268  2163\n    55-59       18685  2992\n    60-64       19239  4022\n    65-69       17849  4622\n    70-74       12895  3598\n    75-79        8877  2341\n    80 or older  9963  2284\n\n$Education\n                        train_data$Diabetes_binary\ntrain_data[[x]]             No   Yes\n  Up to some high school  7119  2456\n  High school graduate   36323  7772\n  Some college           41493  7206\n  College graduate       67898  7308\n\n$Income\n                 train_data$Diabetes_binary\ntrain_data[[x]]      No   Yes\n  &lt;$20,000        20142  6266\n  $20,000-$25,000 11217  2825\n  $25,000-$35,000 14946  3176\n  $35,000-$50,000 21802  3714\n  $50,000-$75,000 26608  3714\n  &gt;$75,000        58118  5047\n\ncross_tabs &lt;- lapply(names(test_data |&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary))), function(x) xtabs(~ test_data[[x]] + test_data$Diabetes_binary))\n\nnames(cross_tabs) &lt;- names(test_data |&gt; select(-c(BMI, PhysHlth, MentHlth, Diabetes_binary)))\ncross_tabs\n\n$HighBP\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  40994  2595\n           Yes 24507  8009\n\n$HighChol\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  40402  3485\n           Yes 25099  7119\n\n$CholCheck\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No   2808    70\n           Yes 62693 10534\n\n$Smoker\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  37438  5138\n           Yes 28063  5466\n\n$Stroke\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  63364  9649\n           Yes  2137   955\n\n$HeartDiseaseorAttack\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  60767  8276\n           Yes  4734  2328\n\n$PhysActivity\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  14498  3889\n           Yes 51003  6715\n\n$Fruits\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  23403  4358\n           Yes 42098  6246\n\n$Veggies\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  11733  2606\n           Yes 53768  7998\n\n$HvyAlcoholConsump\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  61434 10359\n           Yes  4067   245\n\n$AnyHealthcare\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No   3282   450\n           Yes 62219 10154\n\n$NoDocbcCost\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  60263  9460\n           Yes  5238  1144\n\n$GenHlth\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n     excellent 13338   328\n     very good 24908  1875\n     good      18576  4083\n     fair       6402  2929\n     poor       2277  1389\n\n$DiffWalk\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n           No  56809  6657\n           Yes  8692  3947\n\n$Sex\n              test_data$Diabetes_binary\ntest_data[[x]]    No   Yes\n        female 37014  5538\n        male   28487  5066\n\n$Age\n              test_data$Diabetes_binary\ntest_data[[x]]   No  Yes\n   18-24       1694   35\n   25-29       2305   41\n   30-34       3287   83\n   35-39       3972  194\n   40-44       4472  329\n   45-49       5482  549\n   50-54       6958  925\n   55-59       7884 1271\n   60-64       8272 1711\n   65-69       7787 1936\n   70-74       5497 1543\n   75-79       3700 1062\n   80 or older 4191  925\n\n$Education\n                        test_data$Diabetes_binary\ntest_data[[x]]              No   Yes\n  Up to some high school  3050  1070\n  High school graduate   15361  3294\n  Some college           18063  3148\n  College graduate       29027  3092\n\n$Income\n                 test_data$Diabetes_binary\ntest_data[[x]]       No   Yes\n  &lt;$20,000         8409  2771\n  $20,000-$25,000  4864  1229\n  $25,000-$35,000  6433  1328\n  $35,000-$50,000  9377  1577\n  $50,000-$75,000 11346  1551\n  &gt;$75,000        25072  2148\n\n\n\ntrain_data |&gt; select(c(Diabetes_binary, PhysHlth)) |&gt;\n  filter(PhysHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = PhysHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary PhysHlth_med PhysHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          10.6\n2 Yes                       14          15.1\n\ntest_data |&gt; select(c(Diabetes_binary, PhysHlth)) |&gt;\n  filter(PhysHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = PhysHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary PhysHlth_med PhysHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          10.6\n2 Yes                       14          15.2\n\ntrain_data |&gt; select(c(Diabetes_binary, MentHlth)) |&gt;\n  filter(MentHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = MentHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary MentHlth_med MentHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          9.82\n2 Yes                       10         13.2 \n\ntest_data |&gt; select(c(Diabetes_binary, MentHlth)) |&gt;\n  filter(MentHlth &gt; 0) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarise(\n    across(\n      .cols = MentHlth, \n      .fns = list(\n        med  = median,                  \n        mean = mean)))\n\n# A tibble: 2 × 3\n  Diabetes_binary MentHlth_med MentHlth_mean\n  &lt;fct&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n1 No                         5          9.90\n2 Yes                       10         13.3 \n\n\nThese distributions are okay, so we will proceed with this seed."
  },
  {
    "objectID": "Modeling.html#fitting-models",
    "href": "Modeling.html#fitting-models",
    "title": "Modeling",
    "section": "Fitting models",
    "text": "Fitting models\nCreating basic recipe with all variables as predictors. No need to create dummy variables or normalize since we will be using tree models. There also is not a need to create new or combine variables.\n\nrecipe &lt;- recipe(Diabetes_binary ~ ., data = train_data)\n\n\nClassification Trees\n\nExplaination\nA classification tree is a type of decision tree used for predicting categorical outcomes. It works by examining all the predictor variables and determining the best split for each variable at each stage. The algorithm then selects the split that optimally separates the data. This process continues recursively, creating branches until a stopping condition is met. In our model, we are tuning several hyperparameters: tree_depth, which controls the maximum number of branches; cost_complexity, which penalizes overly complex trees to prevent overfitting; and min_n, which sets the minimum number of observations required for a split.\nClassification trees have several advantages. They handle non-linear relationships and categorical predictors effectively because they make decisions based on splits in the data rather than assuming a specific mathematical relationship between the predictors and the response.\nHowever, classification trees are considered fragile models because the choice of the best split can be heavily influenced by individual observations. We mitigate this vulnerability using cross-validation, which builds trees on different subsets of the training data to evaluate stability and performance. Another limitation is that trees can be highly correlated: if a particular variable strongly influences the outcome, many trees may produce the same split, leading to similar predictions across models.\n\n\nThe model\nCreating model and workflow\n\ntree_mod &lt;- decision_tree(tree_depth = tune(), cost_complexity = tune(), min_n = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\ntree_wf &lt;- workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(tree_mod)\n\nCreating tuning grid. The 5 levels for each hyperparameter gives a good balance between exploring the hyperparameter space and keeping computational time reasonable.\n\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = c(5, 5, 5))\n\nFitting the model. Saving the predictions in order to find log loss metric\n\ntree_fits &lt;- tree_wf |&gt;\n  tune_grid(resamples = cv_folds, grid = tree_grid,\n                          control = control_grid(save_pred = TRUE),\n                          metrics = metric_set(mn_log_loss))\n\nSelected the best fit based on the mean log loss and finalizing worflow\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\ntree_final_wf &lt;- tree_wf |&gt;\n  finalize_workflow(tree_best_params)\n\nCreating the final fit based on all the data\n\ntree_final_fit &lt;- tree_final_wf |&gt;\n  last_fit(data_split)\n\nManually calculating the mean log loss\n\nfinal_tree_log_loss &lt;- tree_final_fit |&gt;\n  collect_predictions() |&gt;\n  mn_log_loss(\n    truth = Diabetes_binary,\n    .pred_Yes\n  )\n\n\n\n\nRandom Forest\n\nExplaination\nA random forest is an ensemble method that builds many classification trees and combines their predictions to produce a more stable and accurate model. It addresses the fragility of individual trees in two ways:\n\nBagging: Each tree is trained on a different random sample of the data, drawn with replacement, which reduces the influence of any single observation on the overall model.\nRandom selection of predictors: Each tree is built on a random subset of predictors. This reduces correlation between trees and ensures that no single strong predictor dominates the model.\n\nBy aggregating the predictions of many trees, a random forest is less sensitive to noise in the data and more robust than a single classification tree. Additionally, it provides a measure of variable importance, showing which predictors contribute most to the model’s accuracy.\n\n\nThe model\nCreating model and workflow. Tuning on the minimum number of observations in a branch and the number of predictors that are sampled. Setting the number of trees the forest produces to 500. The importance of the predictors is being tracked. While it is not as important for this model fit, we can apply the results of predictor importance to future models.\n\ncores &lt;- parallel::detectCores()\nrf_mod &lt;- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 500) |&gt;\n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(rf_mod)\n\nFitting the data\n\nrf_fits &lt;- \n  rf_wf |&gt;\n  tune_grid(resample = cv_folds,\n            grid = 15,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n\nChoosing the model with the lowest mean log loss and finalizing workflow\n\nrf_best_params &lt;- select_best(rf_fits, metric = \"mn_log_loss\")\nrf_final_wf &lt;- rf_wf |&gt;\n  finalize_workflow(rf_best_params)\n\nFitting this workflow to all the data to see how well it does with new observations.\n\nrf_final_fit &lt;- rf_final_wf |&gt;\n  last_fit(data_split)\n\nCalculating mean log loss\n\nfinal_rf_log_loss &lt;- rf_final_fit |&gt;\n  collect_predictions() |&gt;\n  mn_log_loss(\n    truth = Diabetes_binary,\n    .pred_Yes\n  )"
  },
  {
    "objectID": "Modeling.html#compaing-models",
    "href": "Modeling.html#compaing-models",
    "title": "Modeling",
    "section": "Compaing models",
    "text": "Compaing models\nSaving the final fits and workflows so that do not need to re-run either of these fits\n\nsave(tree_final_fit, tree_final_wf, file=\"savedtree.RData\")\nsave(rf_final_fit, rf_final_wf, file=\"savedrf.RData\")\n\nFinding variable importance\n\nrf_final_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  vip(num_features = 22)\n\n\n\n\n\n\n\n\nBased on this importance graph, if there are future analyses with new data, I would update the fits by paring down the predictors. I would only include BMI, General Health, Age, HighBP, Physical Health, High Chol, Income, Mental Health, DiffWalk, Education, and HeartDiseaseAttak. This will reduce the model by half while not taking away much precision.\nComparing the mean log loss for the models\n\nfinal_tree_log_loss\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.38\n\nfinal_rf_log_loss\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.52\n\n\nThe mean log loss is lower for the tree model, so we will use this going forward."
  }
]